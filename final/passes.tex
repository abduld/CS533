\subsubsection{Compiler Passes}

The most important factor in parallel computing is how to manage memory
transfer. If a node computes a chunk of data and it is used in
subsequent instructions, the node should reuse the output rather than
send and request the data again. There are two approaches to facilitate
this. The first is a runtime approach: Hadoop, for example, dispatches
tasks to maximize reuse of local data. This done via the Hadoop
scheduler which has a mapping between nodes and data state.

Since the final generated code is C/CUDA, we can rely on the backend compiler
	to perform optimizations which we did not have time
	to implement.
Note that this assumption is not valid for generated
	JavaScript code, but if high-performance JavaScript is needed
        some optimizations
	can be delegated to offline JavaScript optimizers such as the
	Google Closure\cite{bolin2010closure} compiler.

\subsubsubsection{Peephole Optimization}

When lowering the AST to IR, many temporary variables are generated.
They are removed through a peephole optimizer that has removal of
	temporary variables as a pattern.

\subsubsubsection{Def/Use Analysis}

The Def/Use analysis are two passes that compute which variables are defined
	or used at each program point.
The passes also define the life span of variables.
This is a simple pass, since the compiler considers the IR to be in SSA form
 	at this stage --- and while this is a fair assumption considering our
	language as purely functional, we do not have a pass to enforce this currently.

\subsubsubsection{Free Variables}

Free variables are ones which are not bound by the current scope.
This pass computes the set of variables at each statement using
	the Def/Use information.
At each program point, $OUT(p) = \cup_{e \in pred(p)} In(e)$ and $Out(p) = Out(p) \backslash Kill(p) \cup Gen(p)$
	where the $Gen$ set are the use set at that point, and the $Kill$ set is the set
 of definitions.

\subsubsubsection{Closure Conversion}

Since ZOne is a functional language functions are
	first class objects.
This means that some variables inside functions are not
	bound by the function scope.
A compiler pass performs closure conversions (also called
	lambda lifting) to lift the function to the global scope.
This pass uses the free variables pass and is done late,
	since other passes, such as
	redundant code elimination, may be able to remove the
	function statement.

\subsubsubsection{Sharing Analysis}

<<<<<<< HEAD
Sharing analysis determines the data dependency between input and output arrays.
In this work, we focus on only simple lambda functions, which contain only a few statements.
Only the coarse sharing pattern, which means one sharing pattern per array per lambda function, is generated.
All memory accesses associated one array in a lambda function are merged to its coarse sharing pattern.

Our sharing pattern analysis is similar to array reference-based analysis \cite{alg_reference}.
However, our analysis is simplified because the feature of map operation in our language.
Therefore, instead of recording all array reference in all iterations, we only record reference in a subset of iterations, which get referenced, 
since we know all iterations of map are independent. 
In this sense, we dramatically reduce the overhead of analysis.


\subsubsection{Loop Fusion}\label{loop-fusion}

If for example, in the program
\fix{map(f, map(g, lst))} a compiler pass can transform this
into \fix{map(f.g, lst)} where \fix{f.g} is the composition of
\fix{f} and \fix{g}. A simple peephole optimizer can scan for this
instruction pattern and perform this transformation. A generalization of
this technique for other list primitives is found in the the Haskell
vector library. Using a concept called Stream Fusion\cite{StreamFusion}, Haskell
fuses most function loops to remove unnecessary temporaries and list
traversals. In this project, we will adopt some aspects of how Haskell
performs this transformation when they are applicable in the CUDA
programming model --- since GPU programs tend to be memory bound,
reducing the number of temporaries increases performance, for example.

In order to achieve high-performance fusion, a resource estimator and a
memory counter are necessary. The resource estimator monitor on-chip
resource usage for a current tiling to avoid register spilling and drop
of multithreading. The memory counter calculates the potential
memory count saved by fusion. Since 1) the on-chip resources of the GPU are
limited, and 2) the performance gap between on-chip access and off-chip
access on GPU might has less impact than the gap between on-node access
and inter-node access on cluster, the decision of fusion may not be
trivial. Here, we will use a heuristic function of these two parameters to
determine the whether to fuse. A precise resource estimator might be
very difficult to implement and may not really match the real vendor
compiler. In this project, we will simply build a routine to rely on the
feedback of the real vendor compiler after the GPU backend generated the
code.

\subsubsection{Loop Tiling}\label{loop-tiling}

Tiling is critical for performant on GPU programs. In this project,
automatic tiling will be considered as group several single functions
\fix{f}. Considering a function \fix{f} is isolated and map constructs limit
data dependency among different outputs, an analysis can be done for
fix{f} to track data sharing among multiple outputs. Here,
sharing analysis is considered as access pattern analysis of function
\texttt{f}. Therefore, tiling for data sharing is achievable.

Coalesced access of GPU is also important for performance. In this
project coalesced access is easily considered as a specialized type of
tiling. Register packing and thread coarsening of GPU are also
recognized by applying the same analysis, with different tiling.
Therefore, we can potentially perform very aggressive tiling for GPU.

\subsubsection{Autotuning}\label{autotuning}

One of the advantages of compiling from a high level languages into CUDA
is that you can easily tweak parameters for loop tiling, unrolling, and
fusion. A combination of a resource model and autotuning will be used to
maximize the performance of the generated code. We will use a brute-force-like
algorithm, similar to the one employed by ATLAS\cite{atlas} to explore the
parameter space.
