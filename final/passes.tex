\subsubsection{Compiler Passes}

Since C/CUDA code is generated, we rely on the backend compiler
	to perform further optimizations which we did not have time
	to implement.
Note that this assumption is not valid, since we also generate
	JavaScript code, but one can argue that some optimizations
	can be deligated to JavaScript optimizers such as the 
	Google Closure\todo[inline]{cite} compiler. 

\subsubsubsection{Peephole Optimization}

When lowering the AST to IR, many temporary variables are generated.
To remove them we develop a peephole optimizer and insert removal of
	temporary variables as one of our patterns.

\subsubsubsection{Def/Use Analysis}

\todo[inline]{finish me.}

\subsubsubsection{Free Variables}

\todo[inline]{finish me.}

\subsubsubsection{Closure Conversion}

Since ZOne is a functional languages, functions are 
	first class objects.
This means that some variables inside functions are not
	bound by the function scope.
A compiler pass performs closure conversions (also called
	lambda lifting) to lift the function to the global scope.
This pass uses the free variables pass and is done late,
	since other passes, such as 
	redundant code elimination, may be able to remove the
	function statement.

\subsubsubsection{Sharing Analysis}

Sharing analysis determins, based on array accesses, the 
	dependence between arrays.
\todo[inline]{finish me.}


\subsubsection{Loop Fusion}\label{loop-fusion}

The most important factor in parallel computing is how to manage memory
transfer. If a node computes a chunk of data and it is used in
subsequent instructions, then it should reuse the output rather than
send and request the data again. There are two approaches to facilitate
this. The first is a runtime approach: Hadoop, for example, dispatches
tasks to maximize reuse of local data. This done via the Hadoop
scheduler which has a mapping between nodes and data state.

The second is a compiler transformation. This is mainly done via loop
fusion. If for example, one writes a program
\texttt{map(f, map(g, lst))} then a compiler pass can transform this
into \texttt{map(f.g, lst)} where \texttt{f.g} is the composition of
\texttt{f} and \texttt{g}. A simple peephole optimizer can scan for this
instruction pattern and perform this transformation. A generalization of
this technique for other list primitives is found in the the Haskell
vector library. Using a concept called Stream Fusion{[}4{]}, Haskell
fuses most function loops to remove unnecessary temporaries and list
traversals. In this project, we will adopt some aspects of how Haskell
performs this transformation when they are applicable in the CUDA
programming model --- since GPU programs tend to be memory bound,
reducing the number of temporaries increases performance, for example.

In order to achieve high-performance fusion, a resource estimator and a
memory counter are necessary. The resource estimator monitor on-chip
resource usage for a current tiling to avoid register spilling and drop
of multithreading. The memory counter simple calculate the potential
memory count saved for fusion. Since 1) on-chip resource of GPU is
limited, and 2) the performance gap between on-chip access and off-chip
access on GPU might has less impact than the gap between on-node access
and inter-node access on cluster, the decision of fusion may not be
trivial. Here, we will heuristic function of these two parameter to
determine the decision of fusion. A precise resource estimator might be
very difficult to implement and may not really meet the real vendor
compiler. In this project, we will simply build a routine to rely on the
feedback of the real vendor compiler after the GPU backend generated the
code.

\subsubsection{Loop Tiling}\label{loop-tiling}

Tiling is critical for performant on GPU programs. In this project,
automatic tiling will be considered as group several single function
\texttt{f}. Considering function \texttt{f} is isolated and map limit
data dependency among different output, an analysis can be done for
function \texttt{f} to track data sharing among multiple outputs. Here,
sharing analysis is considered as access pattern analysis of function
\texttt{f}. Therefore, tiling for data sharing is achievable.

Coalesced access of GPU is also important for performance. In this
project, coalesced access is easily considered as a specialized type
tiling. Register packing and thread coarsening of GPU are also
recognized by applying the same analysis, with different tiling.
Therefore, we can potentially perform very aggressive tiling for GPU.

\subsubsection{Autotuning}\label{autotuning}

One of the advantages of compiling from a high level languges into CUDA
is that you can easily tweak parameters for loop tiling, unrolling, and
fusion. A combination of a resource model and autotuning will be used to
maximize the performance of the generated code. We will use a bruteforce
like algorithm, similar to the one employed by ATLAS to explore the
parameter space.