\section*{Implementation}

\subsection*{Overview}

ZOne currently generates CUDA code for NVIDIA GPUs. ZOne consists of a custom
runtime that uses Intel Threaded Building Blocks and CUDA Streams to enable
concurrency of host and device I/O, data transfer, and compute. The ZOne parser
uses Parser Combinators for Dart\cite{dartParsers}, and the custom compiler
analysis and transformations are written using the Dart language.

\subsection*{Dart}
Dart is an open-source language developed by Google that is designed for
building scalable web apps. Dart is class-based, single-inheritence,
object-oriented language with a syntax that is similar to C. It supports
interfaces, generics, and abstract classes. Typing is optional - it assists
with static analysis and dynamic runtime checking but does affect the semantics
of the code.

\subsection*{Runtime}
The ZOne runtime is primarily written in C/C++. The ZOne runtime provides all
normal language runtime functions with a focus on hiding I/O latency. The ZOne
compiler provides information to the runtime about what tasks can be
interleaved. These runtime features are built on top of CUDA streams and 
Intel Threaded Building Blocks. The following sections describe the
implementation of the ZOne runtime.

\subsubsection*{CUDA Streams}
By design, CUDA device kernels execute asycnhronously with respect to the
host code. This allows host code to overlap with device code, but does not
allow different device operations (kernels and data transfers) to execute
concurrently. CUDA exposes device concurrency through \textit{CUDA Streams}
\cite{kirk2012programming}.

A CUDA Stream is a sequence of operations that
execute in-order on a CUDA device. Separate streams may be interleaved or
overlap if possible. In this way, it is possible to overlap computation and
memory transfers to hide the latency of some operations.
In order to effectively use streams, CUDA allows synchronization operations
to occur between arbitrary streams and provides host code that allows the
CPU to determine the execution status and progress of different streams.

The flexibility of CUDA streams is limited by the device hardware.
For example, the Fermi architecture can manage one queue of kernels, one queue
of device$\rightarrow$host transfers, and one queue of host$\rightarrow$device
transfers. Stream
dependencies between queues are maintained, but there are no dependencies
within queues - the operations are simply done in the order they are put into
the queue. This allows overlap of data transfer and compute on Fermi GPUs.
On devices that support more than one concurrent compute operation the amount
of concurrency is limited by the execution resources on the device. If both
operations are too large to run concurrently they will be serialized.

\subsubsection*{Intel Threaded Building Blocks}
Intel Threaded Building Blocks \cite{reinders2007intel} (TBB) is a library for
scalable parallel
programming in C++. It provides templates for common parallel programming
patterns and abstracts away the details of synchronization, load-balancing,
and cache optimization. Instead of writing threads the programmer specifies
tasks, and the library maps them onto threads in an efficient manner.


\subsection*{Compiler Passes}

\subsubsection*{}

\subsubsection*{}

\subsubsection*{}

