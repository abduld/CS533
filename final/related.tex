\section*{Related Work}

The work that is related to our project emphasizes expression of 
data-parallelism through higher-order functions and high-level languages. In
this section, we present a selection of related works.

\subsection*{DryadLINQ}
DryadLINQ\cite{yu2008dryadlinq} is a high-level language for data-parallel
computing. It is designed to handle batch operations on large-scale
distributed systems. It combines strongly-typed .NET objects,
general-purpose declarative and imperative statements, and LINQ expressions
into a sequential program that can be debugged with a standard .NET debugger.
The DryadLINQ system automatically transforms the data-parallel portions of the
program into a distributed execution plan.
% TODO: more to write?

\subsection*{Triolet}
Triolet\cite{rodrigues2014triolet} is a programming interface and system for
distributed-memory clusters that handles task decomposition, scheduling, and
communication. Triolet emphasizes algorithmic skeletons to capture parallel
computation patterns and abstract away the details of the implementation.
Triolet presents the programmer with higher-order functions through which
expressed parallelism is automatically distributed across a cluster.

\subsection*{Thrust}
Thrust\cite{thrust} is a parallel algorithms library for C++ resembling the C++
standard library. Thrust code can use CUDA, Intel TBB, or OpenMP as a final
target to enable high performance across a variety of systems.

\subsection*{Copperhead}
Copperhead\cite{copperhead} is a data-parallel version of a subset of Python
that is
interoperable with traditional Python code and Numpy. Copperhead can use CUDA,
Intel TBB, and OpenMP to accelerate operations. The Copperhead runtime
intercepts annotated function calls and transforms them (and their input data)
to the appropriate
CUDA, OpenMP, or TBB constructs before executing them.

\subsection*{Accelerate}
Accelerate\cite{accelerate} is an embedded array language in Haskell for 
high-performance
computing. It allows computations on multi-dimensional arrays to be expressed
through collective higher-order operations such as maps or reductions. It has
a CUDA and OpenCL backend.

\subsection*{NOVA}
NOVA\cite{collins2013nova} is a data-parallel polymorphic, statically-typed
functional language. Parallelism is expressed through higher-order functions
such as scan, reduce, map, permute, gather, slic, and filter. NOVA has a
squential/parallel C backend, and a CUDA backend.

\subsection*{Adaptive Implementation Selection in SkePU}
SkePU\cite{enmyren2010skepu} is a C++ template library for data-parallel
computations on one or more GPUs through CUDA or OpenCL. SkePU programs are
expressed through skeletons derived from higher-order functions. Notably,
SkePU also implements lazy memory copying to avoid unecessary memory transfers.

\subsection*{GPMR}
GPU MapReduce (GPMR)\cite{stuart2011multi} is a  MapReduce library written
for multi-GPU clusters. GPMR programs are expressed through the map and reduce
higher-order functions. GPMR breaks these programs up into a map stage, a sort stage, and a reduce stage, then uses optimizations that reduce communication at
the expense of computation, which is a tradeoff that is well-suited for GPUs.
These optimizations are all based off of the data partitioning that the
programmer selects.
