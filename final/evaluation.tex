\section{Evaluation}

To demonstrate the performance and correctness of our ZOne implementation, we
present three benchmark: convolution, Black-Scholes, European pricing, and histogram.
While all three benefit greatly from our I/O latency interleaving, we show
	that histogram also improves from our compiler optimizations.


\subsection{Convolution}

\begin{figure}
\centering
\includegraphics[scale=0.5]{data/stencil.pdf}
\caption{Speedup of convolution over naive CUDA implementation.
	The $x$-axis is the convolved vector size in bytes.}
\label{fig:stencil}
\centering
\end{figure}

Convolutions has wide applications in both engineering and mathematics.
Depending on the ``kernel'' (or mask), convolution, or stencil as it is sometimes called,  	can be used to approximate a differential operator ---
 a $[-\frac{1}{h}, 0, \frac{1}{h}]$ kernel approximates the gradient operator, for example.
High-performance CPU convolution implementations
involve vectorization and tiling to make full use of cache bandwidth and
execution resources.
The biggest impact from a GPU point of view, however, is the use of fast
	scratch pad memory as a user managed cache.
We therefore see little performance benefits from our compiler passes, and the
	performance primarily stems from interleaving memory copies.
Figure~\ref{fig:stencil} shows the speedup achieved by our runtime for a 1D
	convolution compared to a naive CUDA implementation.


\subsection{Option Pricing}

\subsubsection{Black-Scholes}
Black-Scholes models option pricing by simplifying its formula, and some other stuff....


Coarsening increases register pressure, and degrades performance.


\begin{figure}
\centering
\includegraphics[scale=0.5]{data/blackscholes.pdf}
\caption{Speedup over CUDA version..todo}
\label{fig:blackscholes}
\centering
\end{figure}


\subsubsection{European Options}

\subsection{Binary Image Segmentation}

Binary image segmentation partitions an image into foreground and background
	pixels.
We examine the performance of two state of the art algorithms.
The first is GrowCut\todo[inline]{cite} which uses cellular automaton to flood
	neighbors based on an initial seed.
The second uses a graph cut's push-relabel algorithm to find the minimum
	cut to partition the foreground and background pixels.

\subsubsection{GrowCut}

\todo[inline]{GrowCut}

\subsubsection{GraphCut}

\todo[inline]{GraphCut}

\subsection{Histogram}

\begin{figure}
\centering
\includegraphics[scale=0.5]{data/histogram.pdf}
\caption{Frames per seconds achieved for histogram equalization kernel on a 4K video. The figure compares the tuned GPU implementation to a naive CPU and tuned CPU implementations. The tuned GPU implementation uses thread coarsening to achieve high fps.}
\label{fig:histogram}
\centering
\end{figure}


\begin{figure}
\centering
\includegraphics[scale=0.5]{data/histogramc.pdf}
\caption{The coarsening parameter impacts the performance of the histogram equalization kernel.}
\label{fig:histogramCoarsining}
\centering
\end{figure}

Histograms are a fundamental analysis tool in image and data processing.
Although serial  histogram implementations are very straightforward, efficient parallel histogram implementations are
more involved due to the data-dependent access pattern. 
Atomic memory operations are typically used to enable potential parallelism for data-dependent stores. 
Privatization, where individual histograms for portions of the data are computed separately
and then compiled together into the overall result, may potentially reduce memory contention and then penalty of serialization.
Also, scan or sort operations on input data can potentially replace atomic memory operations, and achieve the similar functionality.

In this image histogram equalization benchmark, atomic memory operations are specifically applied. 
Figure \ref{fig:histogram} shows the performance difference among serial CPU, efficient parallel CPU, and efficient parallel GPU versions of image histogram equalization on a 4K video.
Our GPU image histogram equalization can achieve up to 260.2 fps on a Tesla C2050, while serial and parallel CPU versions only have 39.1 and 120.5 fps, respectively, on a Intel Xeon E5520 CPU.
Tiling and thread coarsening are the two major optimization involved in this evaulation.
In the histogram kernel particularly, tiling (baseline in Figure \ref{fig:histogramCoarsining}) is simply applied for further optimization in order to gain memory efficiency.
Beyond tiling, thread coarsening (simple coarsening in Figure \ref{fig:histogramCoarsining}) can be applied for hiding the overhead of kernels and give a 1.78x speedup over the tiling version.
An aggressive thread coarsening version (aggressive coarsening in Figure \ref{fig:histogramCoarsining}), which involves loop fusion of independent histograms, can further have up to 3.10x over the tiling version.


