\section*{Introduction}

With the advent of personal digital devices, big data has become an
issue faced not only by large companies but by regular users. Processing
this data using traditional languages is both inefficient and sometimes
not feasible. By their nature, for many tasks, computing on big data is
a highly parallel and scalable and a multitude of solutions have been
proposed to make writing programs to process big data more manageable.
One of the most successful, in terms of deployment, is the
Map-Reduce\cite{MapReduce} programming style (an example is Hadoop
\cite{Hadoop}) which
all big-data companies employ in some way. The problem with this
programming style is that many programming patterns cannot be easily
expressed through it.

The goal of this project is to investigate what is needed from a compiler and
architecture perspective to make personal computing with large data both
efficient and practical. This investigation targets four areas:
\begin{itemize}
\item Language development: What features and restrictions could a
      programming language have to enable useful parallelism?
\item Compiler IR: What information could the IR contain to enable
      optimization of parallel code?
\item Compiler transformations: What transformations are enabled by
      the langauge and IR choices?
\item Runtime: How can the runtime be designed to support computation
      on large datasets?
\end{itemize}

We have developed ZOne, a new programming language, to explore
these questions. ZOne's compiler IR include \texttt{map} and 
\texttt{reduce} instructions to effectively map computation across
CPU and GPU cores. Compiler transformations include loop fusion,
loop tiling, and autotuning for improved performance on both CPU
and GPU architectures. The runtime is designed from the ground up
for asynchronous IO so computation may be overlapped with IO.
